# to test streaming tokens

import os
import asyncio
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

async def stream_chat():
    # ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™” (ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”)
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0.7,
        streaming=True,  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
    prompt = ChatPromptTemplate.from_template(
        "Answer as briefly and concisely as possible. {user_input}"
    )
    
    # ì²´ì¸ ìƒì„±
    chain = prompt | llm
    
    print("ğŸ’¬ í„°ë¯¸ë„ ì±—ë´‡ ì‹œì‘! (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
    print("-" * 50)
    
    while True:
        # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        user_input = input("\nğŸ™‹ You: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
            
        if not user_input:
            continue
            
        print("ğŸ¤– Bot: ", end="", flush=True)
        
        try:
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
            async for chunk in chain.astream({"user_input": user_input}):
                if chunk.content:
                    print(chunk.content, end="", flush=True)
            print()  # ì¤„ë°”ê¿ˆ
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    asyncio.run(stream_chat())
